{
  "id": "65f091c7a398c95809d27354",
  "name": "Activation Functions: ReLU ",
  "slug": "activation-functions-relu",
  "category": "reference",
  "publishedAt": "2024-03-12T17:33:42.556Z",
  "approvedAt": null,
  "languages": [
    "python"
  ],
  "url": "https://www.codewars.com/kata/65f091c7a398c95809d27354",
  "rank": {
    "id": null,
    "name": null,
    "color": null
  },
  "createdAt": "2024-03-12T17:32:55.929Z",
  "createdBy": {
    "username": "rowanschaefer",
    "url": "https://www.codewars.com/users/rowanschaefer"
  },
  "description": "#### Activation Functions: ReLU \nThe ReLU (Rectified Linear Unit) activation function is a widely used non-linear activation function in neural networks. \n\nIt introduces non-linearity to the network by outputting the input directly if it is positive, and zero otherwise. \n\n#### Formula:\nThe formula for the ReLU activation function is:\n\n```math\nf(x)=max(0,x)\n```\n\n#### Where:\n- `x` is the input to the function.\n- `f(x)` is the output of the function.\n\n#### Your Job: \nWrite a function that takes in a value (x) and computes its ReLU activation.",
  "totalAttempts": 14,
  "totalCompleted": 9,
  "totalStars": 0,
  "voteScore": -5,
  "tags": [
    "Data Science"
  ],
  "contributorsWanted": true,
  "unresolved": {
    "issues": 0,
    "suggestions": 0
  }
}